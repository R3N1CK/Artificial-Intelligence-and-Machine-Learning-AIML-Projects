# -*- coding: utf-8 -*-
"""NLP_Project_Sarcasm_Detection_Questions (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v5Xclo_C6m0suOOvTZkRsa3a_ob_lYQC

# Sarcasm Detection
 **Acknowledgement**

Misra, Rishabh, and Prahal Arora. "Sarcasm Detection using Hybrid Neural Network." arXiv preprint arXiv:1908.07414 (2019).

**Required Files given in below link.**

https://drive.google.com/drive/folders/1xUnF35naPGU63xwRDVGc-DkZ3M8V5mMk

## Install `Tensorflow2.0`
"""

!!pip uninstall tensorflow
!pip install tensorflow==2.0.0

"""## Get Required Files from Drive"""

from google.colab import drive
drive.mount('/content/drive/')

#Set your project path 
project_path = "/content/drive/My Drive/Project"

"""#**## Reading and Exploring Data**

## Read Data "Sarcasm_Headlines_Dataset.json". Explore the data and get  some insights about the data. ( 4 marks)
Hint - As its in json format you need to use pandas.read_json function. Give paraemeter lines = True.
"""

import pandas as pd

df = pd.read_json('/content/drive/My Drive/Project/Sarcasm Detection/Data/Sarcasm_Headlines_Dataset.json',lines=True)

df.head(10)

"""## Drop `article_link` from dataset. ( 2 marks)
As we only need headline text data and is_sarcastic column for this project. We can drop artical link column here.
"""

df = df.drop('article_link',axis=1)
df

"""## Get the Length of each line and find the maximum length. ( 4 marks)
As different lines are of different length. We need to pad the our sequences using the max length.
"""

HL = pd.Series(df.headline)
l = HL.str.len()
l

l.max()

"""#**## Modelling**

## Import required modules required for modelling.
"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D
from tensorflow.keras.models import Model, Sequential

"""# Set Different Parameters for the model. ( 2 marks)"""

max_features = 10000
maxlen = 254
embedding_size = 200

"""## Apply Keras Tokenizer of headline column of your data.  ( 4 marks)
Hint - First create a tokenizer instance using Tokenizer(num_words=max_features) 
And then fit this tokenizer instance on your data column df['headline'] using .fit_on_texts()
"""

tokenizer = Tokenizer(num_words=max_features)
text = df.headline
tokenizer.fit_on_texts(text)

"""# Define X and y for your model."""

X = tokenizer.texts_to_sequences(df['headline'])
X = pad_sequences(X, maxlen = maxlen)
y = np.asarray(df['is_sarcastic'])

print("Number of Samples:", len(X))
print(X[0])
print("Number of Labels: ", len(y))
print(y[0])

"""## Get the Vocabulary size ( 2 marks)
Hint : You can use tokenizer.word_index.
"""

wordindex = tokenizer.word_index
wordindex

len(wordindex)

"""#**## Word Embedding**

## Get Glove Word Embeddings
"""

glove_file = project_path + "/Sarcasm Detection/Data/glove.6B.zip"

#Extract Glove embedding zip file
from zipfile import ZipFile
with ZipFile(glove_file, 'r') as z:
  z.extractall()

"""# Get the Word Embeddings using Embedding file as given below."""

EMBEDDING_FILE = './glove.6B.200d.txt'

embeddings = {}
for o in open(EMBEDDING_FILE):
    word = o.split(" ")[0]
    # print(word)
    embd = o.split(" ")[1:]
    embd = np.asarray(embd, dtype='float32')
    # print(embd)
    embeddings[word] = embd

"""# Create a weight matrix for words in training docs"""

num_words = min(max_features, len(wordindex)) + 1

embedding_matrix = np.zeros((num_words, 200))

for word,i in wordindex.items():
    if i>=max_features:
        continue
    embedding_vector = embeddings.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector


len(embeddings.values())

"""## Create and Compile your Model  ( 7 marks)
Hint - Use Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, then dense and dropout layers as required. 
In the end add a final dense layer with sigmoid activation for binary classification.
"""

model = Sequential()
model.add(Embedding(num_words, embedding_size, weights = [embedding_matrix]))
model.add(Bidirectional(LSTM(128, return_sequences = True)))
model.add(GlobalMaxPool1D())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])

"""# Fit your model with a batch size of 100 and validation_split = 0.2. and state the validation accuracy ( 5 marks)"""

batch_size = 100
epochs = 5

model.fit(X, y, batch_size = batch_size, epochs = epochs, validation_split = 0.2)

accuracy = model.evaluate(X,y)
print("Accuracy of the model = %.2f%%" % (accuracy[1]*100))