# -*- coding: utf-8 -*-
"""SeqNLP_Project1_Questions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/135hzCLaJWLA1vn72i0rDSqammtK3eoPI

# Sentiment Classification

## Loading the dataset
"""

from keras.datasets import imdb

vocab_size = 10000 #vocab size

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency.

from keras.preprocessing.sequence import pad_sequences
vocab_size = 10000 #vocab size
maxlen = 300  #number of word used from each review

"""## Train test split"""

#load dataset as a list of ints
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)
#make all sequences of the same length
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test =  pad_sequences(x_test, maxlen=maxlen)

"""## Creating  word index and  key-value pair for word and word_id"""

word_id = imdb.get_word_index()

word_id = {k:(v+3) for k,v in word_id.items()}
word_id["<PAD>"] = 0
word_id["<START>"] = 1
word_id["<UNK>"] = 2

id_word = {value:key for key,value in word_id.items()}
print(' '.join(id_word[id] for id in x_train[0] ))

"""## Build Keras Embedding Layer Model
We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:

* The embedding layer can be used at the start of a larger deep learning model. 
* Also we could load pre-train word embeddings into the embedding layer when we create our model.
* Use the embedding layer to train our own word2vec models.

The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).
"""

from keras.models import Sequential
from keras.layers import Embedding, Dense, LSTM

embedding_length = 32 
MODEL = Sequential() 
MODEL.add(Embedding(vocab_size, embedding_length, input_length=maxlen)) 
MODEL.add(LSTM(100)) 
MODEL.add(Dense(1, activation='sigmoid')) 
print(MODEL.summary())

MODEL.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

MODEL.fit(x_train, y_train, validation_data=(x_test, y_test), epochs= 5, batch_size= 32)

"""## Printing the accuracy"""

acc = MODEL.evaluate(x_test, y_test) 
print("Accuracy = %.2f%%" % (acc[1]*100))

"""## Retrive the output of each layer in keras for a given single test sample from the trained model you built"""

from keras import backend as K
 
 outputs = []
for layer in MODEL.layers:
    keras_function = K.function([MODEL.input], [layer.output])
    outputs.append(keras_function([x_test, 1]))
print(outputs)

"""## Prediction of a test sample"""

for review in [x_test[1000]]:
    temp = []
    temp_padded = pad_sequences([temp], maxlen=maxlen) 
    print(" Sentiment: %s" % (MODEL.predict(([temp_padded][0]))[0][0]))

"""### The model predicts a sesntiment values which lies between 0.0 and 1.0.
### Sentiment value = Close to 0.0 : Review = Negative
### Sentiment value = Close to 1.0 : Review = Positive

## Prediction on user input.
"""

negative = "the movie is very boring"
positive = "a good movie very entertaining"
for review in [negative,positive]:
    temp = []
    for word in review.split(" "):
        temp.append(word_id[word])
    temp_padded = pad_sequences([temp], maxlen=maxlen) 
    print("%s. Sentiment: %s" % (review,MODEL.predict(([temp_padded][0]))[0][0]))

"""### Sentiment value : 0.19 . Hence, Review : Negative
### Sentiment value : 0.89 . Hence, Review : Positive
"""