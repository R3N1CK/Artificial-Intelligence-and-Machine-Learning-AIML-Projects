{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqNLP_Project1_Questions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "# Sentiment Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wq4RCyyPSYRp"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGCtiXUhSWss",
        "outputId": "c5097e7d-4d2e-4e78-bbc7-33211c15a489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "vocab_size = 10000 #vocab size\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "17473536/17464789 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCPC_WN-eCyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 10000 #vocab size\n",
        "maxlen = 300  #number of word used from each review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMEsHYrWxdtk",
        "colab_type": "text"
      },
      "source": [
        "## Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0g381XzeCyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset as a list of ints\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "#make all sequences of the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nnrc1njM8YcG"
      },
      "source": [
        "## Creating  word index and  key-value pair for word and word_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZhMAgaNeCy5",
        "colab_type": "code",
        "outputId": "9ddf2747-36c9-4c34-e310-979ddda5b303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "word_id = imdb.get_word_index()\n",
        "\n",
        "word_id = {k:(v+3) for k,v in word_id.items()}\n",
        "word_id[\"<PAD>\"] = 0\n",
        "word_id[\"<START>\"] = 1\n",
        "word_id[\"<UNK>\"] = 2\n",
        "\n",
        "id_word = {value:key for key,value in word_id.items()}\n",
        "print(' '.join(id_word[id] for id in x_train[0] ))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n",
            "1654784/1641221 [==============================] - 1s 1us/step\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dybtUgUReCy8",
        "colab_type": "text"
      },
      "source": [
        "## Build Keras Embedding Layer Model\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OLM4eBeCy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxNDNhrseCzA",
        "colab_type": "code",
        "outputId": "86af65e0-7b49-4a26-f4a4-9f1b60f90c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "embedding_length = 32 \n",
        "MODEL = Sequential() \n",
        "MODEL.add(Embedding(vocab_size, embedding_length, input_length=maxlen)) \n",
        "MODEL.add(LSTM(100)) \n",
        "MODEL.add(Dense(1, activation='sigmoid')) \n",
        "print(MODEL.summary()) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 300, 32)           320000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 373,301\n",
            "Trainable params: 373,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIHWjn02RzCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERSqY9UHzmHy",
        "colab_type": "code",
        "outputId": "e7bd332d-7376-459a-c5fd-7f149899d43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "MODEL.fit(x_train, y_train, validation_data=(x_test, y_test), epochs= 5, batch_size= 32) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/5\n",
            "25000/25000 [==============================] - 312s 12ms/step - loss: 0.4441 - accuracy: 0.7968 - val_loss: 0.4287 - val_accuracy: 0.8157\n",
            "Epoch 2/5\n",
            "25000/25000 [==============================] - 308s 12ms/step - loss: 0.5211 - accuracy: 0.7444 - val_loss: 0.3833 - val_accuracy: 0.8358\n",
            "Epoch 3/5\n",
            "25000/25000 [==============================] - 309s 12ms/step - loss: 0.3407 - accuracy: 0.8558 - val_loss: 0.3553 - val_accuracy: 0.8475\n",
            "Epoch 4/5\n",
            "25000/25000 [==============================] - 308s 12ms/step - loss: 0.2313 - accuracy: 0.9098 - val_loss: 0.3240 - val_accuracy: 0.8703\n",
            "Epoch 5/5\n",
            "25000/25000 [==============================] - 308s 12ms/step - loss: 0.1850 - accuracy: 0.9316 - val_loss: 0.3433 - val_accuracy: 0.8647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fdd908cbf50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hmW1zNY98w7M"
      },
      "source": [
        "## Printing the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbVvkfGM99ng",
        "colab_type": "code",
        "outputId": "f36d5c40-7c27-48b4-a1d3-4fa28ce766d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "acc = MODEL.evaluate(x_test, y_test) \n",
        "print(\"Accuracy = %.2f%%\" % (acc[1]*100))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 43s 2ms/step\n",
            "Accuracy = 86.47%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igq8Qm8GeCzG",
        "colab_type": "text"
      },
      "source": [
        "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scYwOWka_PV5",
        "colab_type": "code",
        "outputId": "0a4f4dda-6074-4a24-b9b5-be85fa5afdbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " from keras import backend as K\n",
        " \n",
        " outputs = []\n",
        "for layer in MODEL.layers:\n",
        "    keras_function = K.function([MODEL.input], [layer.output])\n",
        "    outputs.append(keras_function([x_test, 1]))\n",
        "print(outputs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[array([[[ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        [ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        [ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        ...,\n",
            "        [-0.01951145,  0.00069703, -0.03557115, ..., -0.03832467,\n",
            "         -0.04537717,  0.03077606],\n",
            "        [-0.01228778,  0.00046838, -0.01154031, ..., -0.00282057,\n",
            "          0.02917405,  0.03288529],\n",
            "        [ 0.14674443, -0.03639597,  0.04794239, ...,  0.01799275,\n",
            "          0.11272208,  0.12595624]],\n",
            "\n",
            "       [[ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        [ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        [ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        ...,\n",
            "        [-0.0408451 , -0.02523319, -0.06308564, ...,  0.0444007 ,\n",
            "         -0.04622019, -0.05424167],\n",
            "        [ 0.06548566,  0.00308217, -0.02480699, ..., -0.06876344,\n",
            "          0.06291126,  0.02027201],\n",
            "        [ 0.10365362, -0.0306631 ,  0.05769318, ..., -0.12644985,\n",
            "          0.03997286,  0.06934112]],\n",
            "\n",
            "       [[ 0.0460784 , -0.06389201, -0.0301482 , ..., -0.08352245,\n",
            "          0.05608809, -0.00675074],\n",
            "        [-0.00829734, -0.00427486, -0.02557123, ..., -0.02172083,\n",
            "         -0.01848263, -0.02138816],\n",
            "        [ 0.07814965, -0.04344901,  0.04011894, ..., -0.06754046,\n",
            "          0.04460458,  0.08302156],\n",
            "        ...,\n",
            "        [ 0.00611843, -0.03585934,  0.07399041, ..., -0.03471497,\n",
            "          0.09378432,  0.05716469],\n",
            "        [-0.01626679,  0.09249552, -0.00815365, ...,  0.06678864,\n",
            "         -0.01888601, -0.07443272],\n",
            "        [-0.17774442,  0.09755182, -0.13315637, ...,  0.14109494,\n",
            "         -0.11618578, -0.09718523]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        [ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        [ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        ...,\n",
            "        [ 0.05227263, -0.02155259, -0.01077234, ...,  0.02177856,\n",
            "         -0.02827826,  0.02958684],\n",
            "        [ 0.07138418, -0.06438466,  0.0777309 , ..., -0.07644239,\n",
            "          0.06822456,  0.09078703],\n",
            "        [-0.03645137,  0.06558909, -0.10669329, ...,  0.06292419,\n",
            "         -0.0572582 , -0.01846747]],\n",
            "\n",
            "       [[ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        [ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        [ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        ...,\n",
            "        [ 0.013967  , -0.06095037,  0.0750657 , ..., -0.0247679 ,\n",
            "          0.05974034,  0.01832855],\n",
            "        [ 0.03512345, -0.04187658, -0.01838487, ..., -0.0003201 ,\n",
            "         -0.01256822,  0.0122774 ],\n",
            "        [-0.23338577,  0.17516267, -0.21954578, ...,  0.1400836 ,\n",
            "         -0.20742218, -0.18444338]],\n",
            "\n",
            "       [[ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        [ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        [ 0.03975408, -0.04225037,  0.04976373, ...,  0.03796664,\n",
            "         -0.00161235, -0.00494425],\n",
            "        ...,\n",
            "        [-0.00682652,  0.02536955,  0.04945336, ..., -0.06235125,\n",
            "         -0.01992825, -0.02099443],\n",
            "        [ 0.15880068, -0.07585293,  0.1588177 , ..., -0.10285526,\n",
            "          0.08340176,  0.1689042 ],\n",
            "        [-0.08852865,  0.03061301, -0.08635589, ...,  0.05021062,\n",
            "         -0.07431541, -0.0232833 ]]], dtype=float32)], [array([[-0.12293309,  0.543703  , -0.4085579 , ..., -0.00773156,\n",
            "        -0.00400308,  0.33601558],\n",
            "       [-0.15011914,  0.18608557,  0.7924225 , ..., -0.01561732,\n",
            "        -0.12169327,  0.570303  ],\n",
            "       [-0.58044237,  0.43235654,  0.09542979, ..., -0.04147751,\n",
            "        -0.01017215,  0.29980156],\n",
            "       ...,\n",
            "       [-0.37625706,  0.5017869 , -0.533456  , ..., -0.02127358,\n",
            "        -0.00263907,  0.3467029 ],\n",
            "       [-0.71944803,  0.49478996, -0.5418061 , ..., -0.05301119,\n",
            "         0.00133456,  0.2658516 ],\n",
            "       [-0.3769415 ,  0.27698123,  0.23263383, ..., -0.03544147,\n",
            "        -0.0419978 ,  0.41037017]], dtype=float32)], [array([[0.06197479],\n",
            "       [0.99436647],\n",
            "       [0.3614641 ],\n",
            "       ...,\n",
            "       [0.03226722],\n",
            "       [0.04467922],\n",
            "       [0.88014615]], dtype=float32)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CNAVINHQ-PQ6"
      },
      "source": [
        "## Prediction of a test sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3zgS9Y17y13",
        "colab_type": "code",
        "outputId": "03850651-a286-4375-9ab9-8cb5a6607b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for review in [x_test[1000]]:\n",
        "    temp = []\n",
        "    temp_padded = pad_sequences([temp], maxlen=maxlen) \n",
        "    print(\" Sentiment: %s\" % (MODEL.predict(([temp_padded][0]))[0][0]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Sentiment: 0.46831185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9LRO8DwK9ZIf"
      },
      "source": [
        "### The model predicts a sesntiment values which lies between 0.0 and 1.0.\n",
        "### Sentiment value = Close to 0.0 : Review = Negative\n",
        "### Sentiment value = Close to 1.0 : Review = Positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i-1ZBPwK-aI-"
      },
      "source": [
        "## Prediction on user input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1JvrToi-Bem",
        "colab_type": "code",
        "outputId": "5d462bb3-1483-4e8f-9f5b-dd814c637707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "negative = \"the movie is very boring\"\n",
        "positive = \"a good movie very entertaining\"\n",
        "for review in [negative,positive]:\n",
        "    temp = []\n",
        "    for word in review.split(\" \"):\n",
        "        temp.append(word_id[word])\n",
        "    temp_padded = pad_sequences([temp], maxlen=maxlen) \n",
        "    print(\"%s. Sentiment: %s\" % (review,MODEL.predict(([temp_padded][0]))[0][0]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the movie is very boring. Sentiment: 0.19278501\n",
            "a good movie very entertaining. Sentiment: 0.8923264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dvVc4qLO-pLw"
      },
      "source": [
        "### Sentiment value : 0.19 . Hence, Review : Negative\n",
        "### Sentiment value : 0.89 . Hence, Review : Positive"
      ]
    }
  ]
}